## Ensemble Machine Learning Models

[![image](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![GitHub repo size](https://img.shields.io/github/repo-size/Data-Science-East-AFrica/EnsembleLearningModel?color=green-yellow&logo=github&logoColor=blue) 
![GitHub language count](https://img.shields.io/github/languages/count/Data-Science-East-AFrica/EnsembleLearningModel?logo=visual-studio-code) 
![GitHub top language](https://img.shields.io/github/languages/top/Data-Science-East-AFrica/EnsembleLearningModel)
![GitHub last commit](https://img.shields.io/github/last-commit/Data-Science-East-AFrica/EnsembleLearningModel?style=plastic&color=brightgreen) 
![Forks](https://img.shields.io/github/forks/Data-Science-East-AFrica/EnsembleLearningModel?style=social)
![GitHub contributors](https://img.shields.io/github/contributors/Data-Science-East-AFrica/EnsembleLearningModel)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://reactjs.org/docs/how-to-contribute.html#your-first-pull-request)
<a href="https://twitter.com/DataScience_Ea"><img src="https://img.shields.io/discord/733027681184251937.svg?style=flat&label=Join%20DSEA%20Community&color=7289DA" alt="Join DSEA"/></a>

[Ensemble models](https://blog.statsbot.co/ensemble-learning-d1dcd548e936) in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. 


Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. That is why ensemble methods placed first in many prestigious machine learning competitions, such as the Netflix Competition, KDD 2009, and Kaggle.



**Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).**


#### **Ensemble methods can be divided into two groups:** 

 - **sequential** ensemble methods where the base learners are generated sequentially (e.g. AdaBoost).
The basic motivation of sequential methods is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.

 - **parallel** ensemble methods where the base learners are generated in parallel (e.g. Random Forest).
The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging.
